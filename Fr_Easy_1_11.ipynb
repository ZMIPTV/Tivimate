{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZMIPTV/Tivimate/blob/main/Fr_Easy_1_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "4qMcEHwd1B4S"
      },
      "outputs": [],
      "source": [
        "#@title Install to Google Drive (for Resuming Training & Automatic Saving)\n",
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.display import clear_output\n",
        "from ipywidgets import Button\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Your drive is not mounted. Creating Fake Drive.\")\n",
        "    os.makedirs('/content/drive/MyDrive')\n",
        "source = \"Rejekts\"\n",
        "!wget https://huggingface.co/{source}/project/resolve/main/project-main.zip -O '/content/project-main.zip' && unzip -n 'project-main.zip' -d /content/drive/MyDrive\n",
        "!cd '/content/drive/MyDrive/project-main' && python download_files.py && pip install -r 'requirements-safe.txt'\n",
        "!pip install pyngrok tensorflow==2.12.0\n",
        "!rm /content/project-main.zip\n",
        "!rm -r /content/sample_data\n",
        "!mkdir -p /content/dataset\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Success\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pjlwm8SOwWVI"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Click this to load a DATASET instead.\n",
        "DATASET = \"NameZip.zip\"  #@param {type:\"string\"}\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "import concurrent.futures\n",
        "import subprocess\n",
        "\n",
        "def sanitize_directory(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            if filename == \".DS_Store\" or filename.startswith(\"._\") or not filename.endswith(('.wav', '.flac', '.mp3', '.ogg', '.m4a')):\n",
        "                os.remove(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            sanitize_directory(file_path)\n",
        "\n",
        "def convert_file(source_file, output_filename_converted):\n",
        "    # Check if the input file is 16-bit\n",
        "    probe_cmd = f'ffprobe -v error -select_streams a:0 -show_entries stream=sample_fmt -of default=noprint_wrappers=1:nokey=1 \"{source_file}\"'\n",
        "    sample_format = subprocess.run(probe_cmd, shell=True, text=True, capture_output=True).stdout.strip()\n",
        "\n",
        "    # Use appropriate ffmpeg command based on sample format\n",
        "    if sample_format == 's16':\n",
        "        # Export as 16-bit WAV\n",
        "        cmd = f'ffmpeg -i \"{source_file}\" -c:a pcm_s16le \"{output_filename_converted}\"'\n",
        "    else:\n",
        "        # Export as 32-bit float WAV (default behavior)\n",
        "        cmd = f'ffmpeg -i \"{source_file}\" -c:a pcm_f32le \"{output_filename_converted}\"'\n",
        "\n",
        "    process = subprocess.run(cmd, shell=True)\n",
        "    if process.returncode != 0:\n",
        "        print(f'Failed to convert {source_file}. The file may be corrupt.')\n",
        "    else:\n",
        "        os.remove(source_file)\n",
        "\n",
        "def convert_audio_files(source_dir, dest_dir):\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        for root, _, files in os.walk(source_dir):\n",
        "            for filename in files:\n",
        "                file_ext = os.path.splitext(filename)[1].lower()\n",
        "                if file_ext in ['.wav', '.flac', '.mp3', '.ogg', '.m4a']:\n",
        "                    source_file = os.path.join(root, filename)\n",
        "                    output_filename = os.path.join(dest_dir, filename)\n",
        "                    output_filename_converted = os.path.splitext(output_filename)[0] + '_converted.wav'\n",
        "                    executor.submit(convert_file, source_file, output_filename_converted)\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/dataset/' + DATASET\n",
        "final_directory = '/content/dataset'\n",
        "temp_directory = '/content/temp_dataset'\n",
        "\n",
        "try:\n",
        "    if os.path.exists(final_directory):\n",
        "        shutil.rmtree(final_directory)\n",
        "        print(\"Dataset folder already found. Wiping clean for import operation...\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in removing the final directory: {e}\")\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(dataset_path):\n",
        "        raise Exception(f'There is no {DATASET} in {os.path.dirname(dataset_path)}')\n",
        "except Exception as e:\n",
        "    print(f\"Error in verifying dataset: {e}\")\n",
        "\n",
        "try:\n",
        "    os.makedirs(final_directory, exist_ok=True)\n",
        "    os.makedirs(temp_directory, exist_ok=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error in creating directories: {e}\")\n",
        "\n",
        "try:\n",
        "    # Unzip data into a temporary directory\n",
        "    !unzip -d {temp_directory} -B {dataset_path}\n",
        "except Exception as e:\n",
        "    print(f\"Error in unzipping data: {e}\")\n",
        "\n",
        "try:\n",
        "    # Sanitize temporary directory\n",
        "    sanitize_directory(temp_directory)\n",
        "except Exception as e:\n",
        "    print(f\"Error in sanitizing directory: {e}\")\n",
        "\n",
        "try:\n",
        "    # Convert audio files and move them to the final directory\n",
        "    convert_audio_files(temp_directory, final_directory)\n",
        "except Exception as e:\n",
        "    print(f\"Error in converting audio files: {e}\")\n",
        "\n",
        "try:\n",
        "    # Clean up temp directory\n",
        "    shutil.rmtree(temp_directory)\n",
        "except Exception as e:\n",
        "    print(f\"Error in removing temp directory: {e}\")\n",
        "\n",
        "try:\n",
        "    # Rename files if needed\n",
        "    !rename 's/(\\w+)\\.(\\w+)~(\\d*)/$1_$3.$2/' {final_directory}/*.*~*\n",
        "except Exception as e:\n",
        "    print(f\"Error in renaming files: {e}\")\n",
        "\n",
        "print('Dataset imported. You can now copy the path of the dataset folder to the training path.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y9fFMU5u1NI5"
      },
      "outputs": [],
      "source": [
        "#@title 1.Preprocess Data\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "model_name = 'My-Voice' #@param {type:\"string\"}\n",
        "#@markdown <small> Enter the path to your dataset folder (a folder with audios of the vocals you will train on), or if you want just upload the audios using the File Manager into the 'dataset' folder.\n",
        "dataset_folder = '/content/dataset' #@param {type:\"string\"}\n",
        "while len(os.listdir(dataset_folder)) < 1:\n",
        "    input(\"Your dataset folder is empty.\")\n",
        "!mkdir -p ./logs/{model_name}\n",
        "with open(f'./logs/{model_name}/preprocess.log','w') as f:\n",
        "    print(\"Starting...\")\n",
        "!python infer/modules/train/preprocess.py {dataset_folder} 40000 2 ./logs/{model_name} False 3.0 > /dev/null 2>&1\n",
        "with open(f'./logs/{model_name}/preprocess.log','r') as f:\n",
        "    if 'end preprocess' in f.read():\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))\n",
        "    else:\n",
        "        print(\"Error preprocessing data... Make sure your dataset folder is correct.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mBg4FshRwlxC"
      },
      "outputs": [],
      "source": [
        "#@title 2.Extract Features\n",
        "f0method = \"rmvpe_gpu\" # @param [\"pm\", \"harvest\", \"rmvpe\", \"rmvpe_gpu\"]\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "with open(f'./logs/{model_name}/extract_f0_feature.log','w') as f:\n",
        "    print(\"Starting...\")\n",
        "if f0method != \"rmvpe_gpu\":\n",
        "    !python infer/modules/train/extract/extract_f0_print.py ./logs/{model_name} 2 {f0method}\n",
        "else:\n",
        "    !python infer/modules/train/extract/extract_f0_rmvpe.py 1 0 0 ./logs/{model_name} True\n",
        "!python infer/modules/train/extract_feature_print.py cuda:0 1 0 0 ./logs/{model_name} v2\n",
        "with open(f'./logs/{model_name}/extract_f0_feature.log','r') as f:\n",
        "    if 'all-feature-done' in f.read():\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))\n",
        "    else:\n",
        "        print(\"Error preprocessing data... Make sure your data was preprocessed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U-wDX7BhwtIv"
      },
      "outputs": [],
      "source": [
        "#@title 3.Train Index\n",
        "import numpy as np\n",
        "import faiss\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "def train_index(exp_dir1, version19):\n",
        "    exp_dir = \"logs/%s\" % (exp_dir1)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    feature_dir = (\n",
        "        \"%s/3_feature256\" % (exp_dir)\n",
        "        if version19 == \"v1\"\n",
        "        else \"%s/3_feature768\" % (exp_dir)\n",
        "    )\n",
        "    if not os.path.exists(feature_dir):\n",
        "        return \"请先进行特征提取!\"\n",
        "    listdir_res = list(os.listdir(feature_dir))\n",
        "    if len(listdir_res) == 0:\n",
        "        return \"请先进行特征提取！\"\n",
        "    infos = []\n",
        "    npys = []\n",
        "    for name in sorted(listdir_res):\n",
        "        phone = np.load(\"%s/%s\" % (feature_dir, name))\n",
        "        npys.append(phone)\n",
        "    big_npy = np.concatenate(npys, 0)\n",
        "    big_npy_idx = np.arange(big_npy.shape[0])\n",
        "    np.random.shuffle(big_npy_idx)\n",
        "    big_npy = big_npy[big_npy_idx]\n",
        "    if big_npy.shape[0] > 2e5:\n",
        "        infos.append(\"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0])\n",
        "        yield \"\\n\".join(infos)\n",
        "        try:\n",
        "            big_npy = (\n",
        "                MiniBatchKMeans(\n",
        "                    n_clusters=10000,\n",
        "                    verbose=True,\n",
        "                    batch_size=256 * config.n_cpu,\n",
        "                    compute_labels=False,\n",
        "                    init=\"random\",\n",
        "                )\n",
        "                .fit(big_npy)\n",
        "                .cluster_centers_\n",
        "            )\n",
        "        except:\n",
        "            info = traceback.format_exc()\n",
        "            logger.info(info)\n",
        "            infos.append(info)\n",
        "            yield \"\\n\".join(infos)\n",
        "\n",
        "    np.save(\"%s/total_fea.npy\" % exp_dir, big_npy)\n",
        "    n_ivf = min(int(16 * np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)\n",
        "    infos.append(\"%s,%s\" % (big_npy.shape, n_ivf))\n",
        "    yield \"\\n\".join(infos)\n",
        "    index = faiss.index_factory(256 if version19 == \"v1\" else 768, \"IVF%s,Flat\" % n_ivf)\n",
        "    infos.append(\"training\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    index_ivf = faiss.extract_index_ivf(index)  #\n",
        "    index_ivf.nprobe = 1\n",
        "    index.train(big_npy)\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/trained_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "\n",
        "    infos.append(\"adding\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    batch_size_add = 8192\n",
        "    for i in range(0, big_npy.shape[0], batch_size_add):\n",
        "        index.add(big_npy[i : i + batch_size_add])\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "    infos.append(\n",
        "        \"成功构建索引，added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (n_ivf, index_ivf.nprobe, exp_dir1, version19)\n",
        "    )\n",
        "\n",
        "training_log = train_index(model_name, 'v2')\n",
        "\n",
        "for line in training_log:\n",
        "    print(line)\n",
        "    if 'adding' in line:\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LnMrksQI1lnW"
      },
      "outputs": [],
      "source": [
        "#@title 4.Train Model\n",
        "#@markdown <small> Enter your ngrok authtoken to open tensorboard. Get one here: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok_authtoken = \"\"#@param {type:\"string\"}\n",
        "!ngrok config add-authtoken {ngrok_authtoken}\n",
        "#%cd /content/drive/MyDrive/project-main\n",
        "from random import shuffle\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "from pyngrok import ngrok\n",
        "now_dir=os.getcwd()\n",
        "#@markdown <small> Enter the name of your model again. It must be the same you chose before.\n",
        "model_name = 'My-Voice'#@param {type:\"string\"}\n",
        "#@markdown <small> Choose how often to save the model and how much training you want it to have.\n",
        "save_frequency = 20 # @param {type:\"slider\", min:5, max:50, step:5}\n",
        "epochs = 420 # @param {type:\"slider\", min:10, max:1000, step:10}\n",
        "#@markdown <small> ONLY cache datasets under 10 minutes long. Otherwise leave this unchecked.\n",
        "cache = False #@param {type:\"boolean\"}\n",
        "# Remove the logging setup\n",
        "\n",
        "def click_train(\n",
        "    exp_dir1,\n",
        "    sr2,\n",
        "    if_f0_3,\n",
        "    spk_id5,\n",
        "    save_epoch10,\n",
        "    total_epoch11,\n",
        "    batch_size12,\n",
        "    if_save_latest13,\n",
        "    pretrained_G14,\n",
        "    pretrained_D15,\n",
        "    gpus16,\n",
        "    if_cache_gpu17,\n",
        "    if_save_every_weights18,\n",
        "    version19,\n",
        "):\n",
        "    # 生成filelist\n",
        "    exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    gt_wavs_dir = \"%s/0_gt_wavs\" % (exp_dir)\n",
        "    feature_dir = (\n",
        "        \"%s/3_feature256\" % (exp_dir)\n",
        "        if version19 == \"v1\"\n",
        "        else \"%s/3_feature768\" % (exp_dir)\n",
        "    )\n",
        "    if if_f0_3:\n",
        "        f0_dir = \"%s/2a_f0\" % (exp_dir)\n",
        "        f0nsf_dir = \"%s/2b-f0nsf\" % (exp_dir)\n",
        "        names = (\n",
        "            set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(feature_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(f0_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(f0nsf_dir)])\n",
        "        )\n",
        "    else:\n",
        "        names = set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)]) & set(\n",
        "            [name.split(\".\")[0] for name in os.listdir(feature_dir)]\n",
        "        )\n",
        "    opt = []\n",
        "    for name in names:\n",
        "        if if_f0_3:\n",
        "            opt.append(\n",
        "                \"%s/%s.wav|%s/%s.npy|%s/%s.wav.npy|%s/%s.wav.npy|%s\"\n",
        "                % (\n",
        "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    f0_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    f0nsf_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    spk_id5,\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            opt.append(\n",
        "                \"%s/%s.wav|%s/%s.npy|%s\"\n",
        "                % (\n",
        "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    spk_id5,\n",
        "                )\n",
        "            )\n",
        "    fea_dim = 256 if version19 == \"v1\" else 768\n",
        "    if if_f0_3:\n",
        "        for _ in range(2):\n",
        "            opt.append(\n",
        "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s/logs/mute/2a_f0/mute.wav.npy|%s/logs/mute/2b-f0nsf/mute.wav.npy|%s\"\n",
        "                % (now_dir, sr2, now_dir, fea_dim, now_dir, now_dir, spk_id5)\n",
        "            )\n",
        "    else:\n",
        "        for _ in range(2):\n",
        "            opt.append(\n",
        "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s\"\n",
        "                % (now_dir, sr2, now_dir, fea_dim, spk_id5)\n",
        "            )\n",
        "    shuffle(opt)\n",
        "    with open(\"%s/filelist.txt\" % exp_dir, \"w\") as f:\n",
        "        f.write(\"\\n\".join(opt))\n",
        "\n",
        "    # Replace logger.debug, logger.info with print statements\n",
        "    print(\"Write filelist done\")\n",
        "    print(\"Use gpus:\", str(gpus16))\n",
        "    if pretrained_G14 == \"\":\n",
        "        print(\"No pretrained Generator\")\n",
        "    if pretrained_D15 == \"\":\n",
        "        print(\"No pretrained Discriminator\")\n",
        "    if version19 == \"v1\" or sr2 == \"40k\":\n",
        "        config_path = \"configs/v1/%s.json\" % sr2\n",
        "    else:\n",
        "        config_path = \"configs/v2/%s.json\" % sr2\n",
        "    config_save_path = os.path.join(exp_dir, \"config.json\")\n",
        "    if not pathlib.Path(config_save_path).exists():\n",
        "        with open(config_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            with open(config_path, \"r\") as config_file:\n",
        "                config_data = json.load(config_file)\n",
        "                json.dump(\n",
        "                    config_data,\n",
        "                    f,\n",
        "                    ensure_ascii=False,\n",
        "                    indent=4,\n",
        "                    sort_keys=True,\n",
        "                )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    cmd = (\n",
        "        'python infer/modules/train/train.py -e \"%s\" -sr %s -f0 %s -bs %s -g %s -te %s -se %s %s %s -l %s -c %s -sw %s -v %s'\n",
        "        % (\n",
        "            exp_dir1,\n",
        "            sr2,\n",
        "            1 if if_f0_3 else 0,\n",
        "            batch_size12,\n",
        "            gpus16,\n",
        "            total_epoch11,\n",
        "            save_epoch10,\n",
        "            \"-pg %s\" % pretrained_G14 if pretrained_G14 != \"\" else \"\",\n",
        "            \"-pd %s\" % pretrained_D15 if pretrained_D15 != \"\" else \"\",\n",
        "            1 if if_save_latest13 == True else 0,\n",
        "            1 if if_cache_gpu17 == True else 0,\n",
        "            1 if if_save_every_weights18 == True else 0,\n",
        "            version19,\n",
        "        )\n",
        "    )\n",
        "    # Use PIPE to capture the output and error streams\n",
        "    p = Popen(cmd, shell=True, cwd=now_dir, stdout=PIPE, stderr=STDOUT, bufsize=1, universal_newlines=True)\n",
        "\n",
        "    # Print the command's output as it runs\n",
        "    for line in p.stdout:\n",
        "        print(line.strip())\n",
        "\n",
        "    # Wait for the process to finish\n",
        "    p.wait()\n",
        "    return \"训练结束, 您可查看控制台训练日志或实验文件夹下的train.log\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs --port=8888\n",
        "print(\"Tensorboard NGROK URL:\",end=\"\")\n",
        "ngrok_url = ngrok.connect(8888)\n",
        "print(ngrok_url)\n",
        "try:\n",
        "    training_log = click_train(\n",
        "        model_name,\n",
        "        '40k',\n",
        "        True,\n",
        "        0,\n",
        "        save_frequency,\n",
        "        epochs,\n",
        "        12,\n",
        "        True,\n",
        "        'assets/pretrained_v2/f0G40k.pth',\n",
        "        'assets/pretrained_v2/f0D40k.pth',\n",
        "        0,\n",
        "        cache,\n",
        "        True,\n",
        "        'v2',\n",
        "    )\n",
        "    print(training_log)\n",
        "except:\n",
        "    ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}